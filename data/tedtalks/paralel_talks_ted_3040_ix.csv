en,fr,es,title,speaker,duration,tags
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											Chris Anderson: Nick Bostrom.
											So, you have already given us
so many crazy ideas out there.
											I think a couple of decades ago,
											you made the case that we might
all be living in a simulation,
											or perhaps probably were.
											More recently,
											you've painted the most vivid examples
of how artificial general intelligence
											could go horribly wrong.
											And now this year,
											you're about to publish
											a paper that presents something called
the vulnerable world hypothesis.
											And our job this evening is to
give the illustrated guide to that.
											So let's do that.
											What is that hypothesis?
									","
											Chris Anderson : Nick Bostrom.
											Vous nous avez déjà offert
tant d'idées folles.
											Je pense à il y a environ 20 ans,
											vous avez avancé que nous pourrions
tous vivre dans une simulation
											ou que c'était probablement le cas.
											Plus récemment,
											vous avez dépeint les exemples
les plus saisissants
											quant à comment une intelligence
générale artificielle
											pourrait terriblement mal tourner.
											Cette année,
											vous êtes sur le point de publier
											un article qui présente une chose appelée
l'hypothèse du monde vulnérable.
											Notre rôle ce soir est d'offrir
un guide illustré à ce sujet.
											Allons-y.
											Quelle est cette hypothèse ?
									","
											Chris Anderson: Nick Bostrom.
											Nos has dado muchas ideas locas.
											Creo que hace un par de décadas,
											defendías que estaríamos 
viviendo una simulación,
											o que, probablemente, vivíamos en una.
											Recientemente,
											has dado ejemplos muy claros de 
cómo la inteligencia artificial general
											podría generar resultados terribles.
											Y este año estás a punto de publicar
											un ensayo sobre algo denominado
la hipótesis del mundo vulnerable.
											Nuestro trabajo esta tarde consiste
en ofrecer una guía ilustrada sobre eso.
											Así que empecemos.
											¿En qué consiste esa hipótesis?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											Nick Bostrom: It's trying to think about
											a sort of structural feature
of the current human condition.
											You like the urn metaphor,
											so I'm going to use that to explain it.
											So picture a big urn filled with balls
											representing ideas, methods,
possible technologies.
											You can think of the history
of human creativity
											as the process of reaching into this urn
and pulling out one ball after another,
											and the net effect so far
has been hugely beneficial, right?
											We've extracted a great many white balls,
											some various shades of gray,
mixed blessings.
											We haven't so far
pulled out the black ball —
											a technology that invariably destroys
the civilization that discovers it.
											So the paper tries to think
about what could such a black ball be.
									","
											Nick Bostrom :
C'est une tentative de réflexion
											sur une caractéristique structurelle
de la condition humaine actuelle.
											Vous aimez la métaphore de l'urne
											alors je vais l'utiliser pour expliquer.
											Imaginez une grande urne remplie de boules
											représentant des idées, des méthodes,
des technologies possibles.
											Vous pouvez voir l'histoire
de la créativité humaine
											comme le fait de plonger
la main dans l'urne
											et d'en sortir une boule après l'autre,
											et le résultat jusqu'ici
a été extrêmement bénéfique.
											Nous avons extrait
de nombreuses boules blanches,
											quelques grises
dont les bienfaits sont mitigés.
											Jusqu'ici, nous n'avons pas
sorti la boule noire —
											une technologie qui détruit invariablement
la civilisation qui la découvre.
											La publication essaye de réfléchir
à ce qu'une balle noire pourrait être.
									","
											Nick Bostrom: Se trata de pensar
											en una especie de característica
estructural de la actual condición humana.
											Como te gusta la metáfora de la urna,
											voy a utilizarla para explicarlo.
											Imagínate una gran urna llena de bolas
											que representan ideas,
métodos, posibles tecnologías.
											Puedes pensar en la historia
de la creatividad humana
											como el proceso de extraer 
de esta urna una bola tras otra,
											y el efecto neto hasta la fecha
ha sido muy beneficioso, ¿verdad?
											Hemos extraído 
una gran cantidad de bolas blancas,
											de varias tonalidades de gris,
con pros y contras.
											Hasta ahora no hemos sacado la bola negra:
											una tecnología que invariablemente
destruye la civilización que la descubre.
											Mi ensayo trata de pensar
qué podría ser esa bola negra.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So you define that ball
											as one that would inevitably
bring about civilizational destruction.
									","
											CA : Vous définissez cette boule
											comme étant celle menant invariablement
à la destruction de la civilisation.
									","
											CA: Así que defines la bola negra
											como una que llevará inevitablemente 
a la destrucción de la civilización.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Unless we exit what I call
the semi-anarchic default condition.
											But sort of, by default.
									","
											NB : A moins que nous ne sortions
											de ce que j'appelle la condition
semi-anarchique par défaut.
											Mais par défaut.
									","
											NB: Salvo que salgamos de lo que denomino 
condición semianárquica por defecto.
											Bueno, más o menos por defecto.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So, you make the case compelling
											by showing some sort of counterexamples
											where you believe that so far
we've actually got lucky,
											that we might have pulled out
that death ball
											without even knowing it.
											So there's this quote, what's this quote?
									","
											CA : Vous présentez
un argumentaire convaincant
											en montrant des contre-exemples
											où vous croyez que jusqu'ici,
nous avons eu de la chance,
											que nous pourrions avoir sorti
la boule de la mort
											sans même le savoir.
											Il y a cette citation, quelle est-elle ?
									","
											CA: Haces que este caso adquiera
carácter de urgencia
											al mostrar contraejemplos
											que crees que demuestran
que hasta ahora hemos tenido suerte,
											que puede que hayamos sacado 
la bola de la muerte,
											sin habernos siquiera dado cuenta.
											Hay una cita al respecto, ¿cuál es?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Well, I guess
it's just meant to illustrate
											the difficulty of foreseeing
											what basic discoveries will lead to.
											We just don't have that capability.
											Because we have become quite good
at pulling out balls,
											but we don't really have the ability
to put the ball back into the urn, right.
											We can invent, but we can't un-invent.
											So our strategy, such as it is,
											is to hope that there is
no black ball in the urn.
									","
											NB : Je suppose
qu'elle est censée illustrer
											la difficulté liée à la prédiction
											de ce à quoi mèneront
des découvertes élémentaires.
											Nous n'avons simplement pas
cette capacité.
											Car nous sommes devenus bons
dans l'extraction de boules
											mais nous n'avons pas la capacité
de remettre la boule dans l'urne.
											Nous pouvons inventer
mais nous ne pouvons pas désinventer.
											Notre stratégie actuelle
											est d'espérer qu'il n'y pas
de boule noire dans l'urne.
									","
											NB: Bueno, imagino que solo trato
de ilustrar la dificultad de prever
											hacia dónde llevarán
los descubrimientos básicos,
											simplemente no tenemos esa capacidad.
											Porque nos hemos hecho bastante
hábiles extrayendo bolas,
											pero realmente no tenemos la capacidad
de devolver la bola a la urna.
											Podemos inventar, pero no desinventar.
											Así que nuestra estrategia, en realidad,
											es confiar en que no haya
una bola negra en la urna.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So once it's out, it's out,
and you can't put it back in,
											and you think we've been lucky.
											So talk through a couple
of these examples.
											You talk about different
types of vulnerability.
									","
											CA : Une fois sortie, elle est sortie
et vous ne pouvez pas la remettre,
											et vous nous pensez chanceux.
											Expliquez-nous quelques-uns
de ces exemples.
											Vous parlez de différents types
de vulnérabilités.
									","
											CA: Así que una vez que está fuera,
no se puede devolver a la urna,
											y crees que hemos tenido suerte.
											Cuéntanos un par de ejemplos.
											Hablas de diferentes tipos
de vulnerabilidad.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: So the easiest type to understand
											is a technology
that just makes it very easy
											to cause massive amounts of destruction.
											Synthetic biology might be a fecund
source of that kind of black ball,
											but many other possible things we could —
											think of geoengineering,
really great, right?
											We could combat global warming,
											but you don't want it
to get too easy either,
											you don't want any random person
and his grandmother
											to have the ability to radically
alter the earth's climate.
											Or maybe lethal autonomous drones,
											massed-produced, mosquito-sized
killer bot swarms.
											Nanotechnology,
artificial general intelligence.
									","
											NB : Le type le plus simple à comprendre
											est une technologie qui rend
la destruction massive très facile.
											La biologie synthétique pourrait être
une source féconde d'une telle boule,
											mais nombre d'autres choses
que nous pourrions —
											pensez à la géo-ingénierie,
c'est génial, non ?
											Nous pourrions contrer
le réchauffement climatique
											mais nous ne voulons pas
que ce soit trop simple
											et qu'un individu quelconque
et sa grand-mère
											aient la capacité d'altérer radicalement
le climat terrestre.
											Ou peut-être les drones autonomes mortels,
											des essaims de robots tueurs
											de la taille d'un moustique
et produits en masse.
											La nanotechnologie,
l'intelligence générale artificielle.
									","
											NB: El tipo más fácil de comprender
											es una tecnología que hace muy fácil
producir una enorme destrucción.
											La biología sintética podría ser
una fuente abundante
											de ese tipo de bola negra,
											pero hay muchas otras cosas
en las que podríamos pensar,
											como la geoingeniería,
realmente genial, ¿verdad?
											Podríamos combatir
el calentamiento global,
											pero tampoco queremos
que sea demasiado sencillo,
											que cualquier persona
y su abuela tengan la capacidad
											de alterar radicalmente
el clima de la Tierra.
											O tal vez producir en masa 
drones autónomos letales,
											enjambres de robots asesinos
del tamaño de un mosquito.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: You argue in the paper
											that it's a matter of luck
that when we discovered
											that nuclear power could create a bomb,
											it might have been the case
											that you could have created a bomb
											with much easier resources,
accessible to anyone.
									","
											CA : Vous avancez
											que c'est une question de chance
que quand nous avons découvert
											que la puissance nucléaire
pouvait créer une bombe,
											il aurait pu arriver
											que nous puissions créer une bombe
											avec des ressources bien plus simples
d'accès pour quiconque.
									","
											Nanotecnología,
inteligencia artificial general.
											CA: Tú argumentas en el ensayo
que fue una cuestión de suerte
											que el descubrimiento 
de la energía nuclear
											pudiera crear una bomba;
											podría haber sido el caso
de que fuera posible crear una bomba
											con recursos mucho menores,
accesibles a cualquiera.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, so think back to the 1930s
											where for the first time we make
some breakthroughs in nuclear physics,
											some genius figures out that it's possible
to create a nuclear chain reaction
											and then realizes
that this could lead to the bomb.
											And we do some more work,
											it turns out that what you require
to make a nuclear bomb
											is highly enriched uranium or plutonium,
											which are very difficult materials to get.
											You need ultracentrifuges,
											you need reactors, like,
massive amounts of energy.
											But suppose it had turned out instead
											there had been an easy way
to unlock the energy of the atom.
											That maybe by baking sand
in the microwave oven
											or something like that
											you could have created
a nuclear detonation.
											So we know that that's
physically impossible.
											But before you did the relevant physics
											how could you have known
how it would turn out?
									","
											NB : Repensez aux années 30,
											où pour la première fois nous avons fait
											des découvertes capitales
en physique nucléaire,
											un génie découvre qu'il est possible
											d'entraîner une réaction
nucléaire en chaîne
											puis réalise que cela pourrait
mener à la bombe.
											Nous continuons à y travailler
											et il s'avère qu'il vous faut,
pour une bombe nucléaire,
											de l'uranium ou du plutonium
hautement enrichi,
											qui sont très difficiles à obtenir.
											Vous avez besoin d'ultracentrifugeuses,
											de réacteurs et de quantités
massives d'énergie.
											Mais supposez qu'au lieu de cela,
											il y ait eu une façon simple
de libérer l'énergie de l'atome.
											Peut-être qu'en cuisant du sable
dans un four micro-ondes
											ou quelque chose du genre,
											vous auriez pu causer
une détonation nucléaire.
											Nous savons que c'est
physiquement impossible.
											Mais avant de faire les calculs adéquats,
											comment savoir
comment cela allait tourner ?
									","
											NB: Sí, creo que allá por los años 30,
											cuando por primera vez hicimos
grandes avances en física nuclear,
											algunas figuras insignes expusieron
											que era posible crear
reacciones nucleares en cadena
											para luego caer en la cuenta de que eso
podría llevar a crear una bomba.
											Investigamos y descubrimos
que para hacer una bomba nuclear
											se necesita uranio enriquecido o plutonio,
											que son materiales
muy difíciles de conseguir.
											Se precisan ultracentrifugadoras,
											reactores para gigantescas
cantidades de energía.
											Pero imagínate que en lugar de eso,
											hubiera habido una manera fácil
de liberar la energía del átomo.
											Que tal vez calentando arena
en el microondas o algo así
											se hubiese podido crear
una detonación nuclear.
											Sabemos que es físicamente imposible.
											Pero antes de desarrollar
esa física relevante,
											¿cómo se podría haber sabido
cómo acabaría?
											CA: A pesar de eso, ¿no se podría alegar
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Although, couldn't you argue
											that for life to evolve on Earth
											that implied sort of stable environment,
											that if it was possible to create
massive nuclear reactions relatively easy,
											the Earth would never have been stable,
											that we wouldn't be here at all.
									","
											CA : Ne pourriez-vous pas avancer
que pour que la vie évolue sur Terre,
											cela suppose un environnement stable,
											que s'il était possible de causer
des réactions nucléaires massives
											relativement facilement,
											la Terre n'aurait jamais été stable,
											nous ne serions pas là.
									","
											que para que la vida
evolucionara en la Tierra
											se necesitaría cierto entorno estable,
											y que si fuera tan fácil crear
reacciones nucleares masivas
											la Tierra nunca habría sido estable,
											y por tanto no podríamos estar aquí.
											NB: Sí, salvo que hubiera algo fácil 
de hacer a propósito,
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, unless there were something
that is easy to do on purpose
											but that wouldn't happen by random chance.
											So, like things we can easily do,
											we can stack 10 blocks
on top of one another,
											but in nature, you're not going to find,
like, a stack of 10 blocks.
									","
											NB : Oui, à moins qu'il y ait une chose
facile à faire volontairement
											mais qui n'arriverait pas par hasard.
											Des choses faciles à faire,
											comme empiler 10 cubes
les uns sur les autres,
											mais dans la nature, vous ne trouverez pas
une pile de 10 cubes.
									","
											pero que no ocurriría solo por azar.
											Es decir, cosas que podemos
hacer con facilidad,
											podemos apilar 10 bloques uno sobre otro,
											pero en la naturaleza, no se
da una pila de 10 bloques.
											CA: De acuerdo, quizá esto es
lo que más nos preocupa a la mayoría,
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: OK, so this is probably the one
											that many of us worry about most,
											and yes, synthetic biology
is perhaps the quickest route
											that we can foresee
in our near future to get us here.
									","
											CA : C'est probablement la chose
											qui inquiète le plus
la majorité d'entre nous
											et oui, la biologie synthétique
est peut-être le chemin le plus rapide
											qui, nous pouvons envisager,
nous y mènera dans un avenir proche.
									","
											y sí, la biología sintética es 
tal vez la vía más rápida
											que podemos prever en nuestro futuro
cercano para llegar hasta aquí.
											NB: Sí, imagínate lo que eso
habría supuesto,
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, and so think
about what that would have meant
											if, say, anybody by working
in their kitchen for an afternoon
											could destroy a city.
											It's hard to see how
modern civilization as we know it
											could have survived that.
											Because in any population
of a million people,
											there will always be some
who would, for whatever reason,
											choose to use that destructive power.
											So if that apocalyptic residual
											would choose to destroy a city, or worse,
											then cities would get destroyed.
									","
											NB : Réfléchissez
à ce que cela aurait signifié
											si, disons, quiconque travaillant
dans sa cuisine pendant un après-midi
											pouvait détruire une ville.
											Il est difficile de voir comment
la civilisation moderne actuelle
											aurait pu survivre à cela.
											Car dans toute population
d'un million de personnes,
											il y aura toujours quelqu'un
qui, peu importe la raison,
											choisira d'utiliser
ce pouvoir destructeur.
											Si ce résidu apocalyptique
											décidait de détruire une ville, ou pire,
											des villes seraient détruites.
									","
											si, digamos, cualquiera trabajando
en su cocina por la tarde
											pudiera destruir una ciudad.
											Cuesta entender
que la civilización moderna,
											tal como la conocemos,
haya podido sobrevivir a eso,
											porque en cualquier población
de un millón de personas
											siempre habrá alguien que,
por algún motivo,
											quiera utilizar ese poder de destrucción.
											Así que si ese residuo apocalíptico
											eligiera destruir una ciudad, o peor aún,
											las ciudades podrían ser destruidas.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So here's another type
of vulnerability.
											Talk about this.
									","
											CA : Voici un autre type de vulnérabilité.
											Parlez-en.
									","
											CA: Hay otra clase de vulnerabilidad,
											hábleme de ella.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, so in addition to these
kind of obvious types of black balls
											that would just make it possible
to blow up a lot of things,
											other types would act
by creating bad incentives
											for humans to do things that are harmful.
											So, the Type-2a, we might call it that,
											is to think about some technology
that incentivizes great powers
											to use their massive amounts of force
to create destruction.
											So, nuclear weapons were actually
very close to this, right?
											What we did, we spent
over 10 trillion dollars
											to build 70,000 nuclear warheads
											and put them on hair-trigger alert.
											And there were several times
during the Cold War
											we almost blew each other up.
											It's not because a lot of people felt
this would be a great idea,
											let's all spend 10 trillion dollars
to blow ourselves up,
											but the incentives were such
that we were finding ourselves —
											this could have been worse.
											Imagine if there had been
a safe first strike.
											Then it might have been very tricky,
											in a crisis situation,
											to refrain from launching
all their nuclear missiles.
											If nothing else, because you would fear
that the other side might do it.
									","
											NB : En plus de ces types
de balles noires évidents
											qui rendraient possible
de faire exploser plein de choses,
											d'autres types agiraient en créant
de mauvaises incitations
											pour que les humains
fassent des choses nuisibles.
											Le type 2a, nous pouvons l'appeler ainsi,
											c'est réfléchir à une technologie
qui invite les grandes puissances
											à utiliser leurs forces massives
pour causer de la destruction.
											Les armes nucléaires
en étaient très proches.
											Nous avons dépensé
plus de 10 billions de dollars
											pour fabriquer 70 000 ogives nucléaires
											et les placer en état
d'alerte instantanée.
											A plusieurs reprises
durant la Guerre froide,
											on a failli se faire sauter.
											Non pas que beaucoup de gens pensaient
que ce serait une super idée
											de dépenser 10 billions de dollars
pour se faire sauter,
											mais les incitations étaient telles
que nous nous trouvions —
											cela aurait pu être pire.
											Imaginez s'il y a avait eu
un premier coup assuré.
											Cela aurait pu être très délicat,
											dans une situation de crise,
											de s'abstenir de lancer
tous les missiles nucléaires.
											A défaut d'autre chose,
											car vous craindriez
que l'autre camp ne le fasse.
									","
											NB: Sí, además de estos tipos
evidentes de bolas negras
											que podrían hacer que muchas cosas
volaran por los aires,
											hay otros tipos que pueden
crear malos incentivos
											para que los humanos
hagamos cosas dañinas.
											El Tipo 2a, podríamos llamarlo,
											serían tecnologías que posibilitan
a los grandes poderes
											usar su enorme fuerza
para crear destrucción.
											Las armas nucleares
están muy cerca de esto, ¿verdad?
											Hemos gastado más de USD 10 billones
											para construir 70 000 cabezas nucleares
											que están en estado de alerta instantánea.
											Hubo varios momentos
durante la Guerra Fría
											en los que casi nos volamos por los aires.
											No es que muchos pensaran
que era una gran idea
											gastarnos USD 10 billones 
para volarnos por los aires,
											pero los incentivos eran tales
que nos encontrábamos...
											podría haber sido mucho peor.
											Imagina que hubiera habido
un primer ataque seguro.
											Habría sido muy difícil,
en una situación de crisis,
											reprimir el lanzamiento
de todos los misiles nucleares,
											aunque solo fuera por el temor
a que lo hiciera el otro.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Right, mutual assured destruction
											kept the Cold War relatively stable,
											without that, we might not be here now.
									","
											CA : La destruction mutuelle assurée
											a maintenu la Guerre froide
relativement stable.
											Sans cela, nous pourrions ne pas être là.
									","
											CA: De acuerdo, la destrucción
mutua asegurada
											mantuvo la Guerra Fría
relativamente estable,
											porque sin eso,
podríamos no estar hoy aquí.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: It could have been
more unstable than it was.
											And there could be
other properties of technology.
											It could have been harder
to have arms treaties,
											if instead of nuclear weapons
											there had been some smaller thing
or something less distinctive.
									","
											NB : Cela aurait pu être
plus instable que cela.
											Il pourrait y avoir
d'autres propriétés technologiques.
											Il aurait pu être plus dur
d'avoir des traités
											si au lieu des armes nucléaires,
											cela avait été une chose plus petite
ou moins distinctive.
									","
											NB: Pudo haber sido más inestable,
											y pudo haber otras propiedades
de la tecnología,
											pudo haber sido más difícil
lograr tratados armamentísticos,
											si en vez de cabezas nucleares,
											hubiera habido cosas más pequeñas
o menos distintivas.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: And as well as bad incentives
for powerful actors,
											you also worry about bad incentives
for all of us, in Type-2b here.
									","
											CA : Avec les mauvaises incitations
pour les acteurs puissants,
											vous vous inquiétez de telles incitations
pour nous tous, le type 2b.
									","
											CA: Al igual que los malos incentivos
para los actores del poder,
											también te preocupan los malos incentivos
para todos nosotros, en este Tipo 2a.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, so, here we might
take the case of global warming.
											There are a lot of little conveniences
											that cause each one of us to do things
											that individually 
have no significant effect, right?
											But if billions of people do it,
											cumulatively, it has a damaging effect.
											Now, global warming
could have been a lot worse than it is.
											So we have the climate
sensitivity parameter, right.
											It's a parameter that says
how much warmer does it get
											if you emit a certain amount
of greenhouse gases.
											But, suppose that it had been the case
											that with the amount
of greenhouse gases we emitted,
											instead of the temperature rising by, say,
											between three and 4.5 degrees by 2100,
											suppose it had been
15 degrees or 20 degrees.
											Like, then we might have been
in a very bad situation.
											Or suppose that renewable energy
had just been a lot harder to do.
											Or that there had been
more fossil fuels in the ground.
									","
											NB : Nous pourrions prendre le cas
du réchauffement climatique.
											Il y a beaucoup de petits conforts
											qui mènent chacun d'entre nous
à faire des choses
											qui, individuellement,
n'ont pas d'effet significatif.
											Mais si des milliards de gens le font,
											cumulativement, cela a
un effet préjudiciable.
											Le réchauffement climatique
aurait pu être pire qu'il ne l'est.
											Nous avons le paramètre
de sensibilité climatique.
											C'est un paramètre
qui indique le réchauffement
											si on émet une certaine quantité
de gaz à effet de serre.
											Mais supposez que la situation
											soit qu'avec les quantités
de gaz à effet de serre émis,
											au lieu d'une température augmentant
d'entre 3 et 4,5 degrés d'ici 2100,
											imaginez que c'eût été 15 ou 20 degrés.
											La situation aurait alors
été très mauvaise.
											Imaginez que les énergies renouvelables
aient été bien plus complexes.
											Ou qu'il y ait eu
plus de carburants fossiles.
									","
											NB: Sí, podemos poner el caso
del calentamiento global.
											Hay muchos pequeños inconvenientes
											que nos llevan a hacer cosas
											que individualmente
no tienen efecto, ¿verdad?
											Pero si miles de millones
de personas lo hacen,
											acumulativamente,
tiene un efecto perjudicial.
											El calentamiento global
podría haber sido mucho peor.
											Tenemos el parámetro
de la sensibilidad climática.
											Se trata de un parámetro
que indica cuánto se calentaría
											si emitieras cierta cantidad
de gases de efecto invernadero.
											Pero, suponiendo que fuera el caso
											de que con la cantidad de gases
de efecto invernadero que hemos emitido
											en lugar de que la temperatura subiera,
entre 3 y 4.5 º C hasta 2100,
											imagina que hubieran sido 15 o 20 ºC,
											entonces estaríamos
en una situación muy mala.
											O que la energía renovable
fuera más difícil de producir.
											o que hubiera más combustibles
fósiles en el subsuelo.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Couldn't you argue
that if in that case of —
											if what we are doing today
											had resulted in 10 degrees difference
in the time period that we could see,
											actually humanity would have got
off its ass and done something about it.
											We're stupid, but we're not
maybe that stupid.
											Or maybe we are.
									","
											CA : Ne pourriez-vous pas avancer
que dans ce cas de —
											si ce que nous faisons aujourd'hui
											avait entraîné une différence de 10 degrés
sur une période que nous pourrions voir,
											l'humanité se serait bougé le cul
et y aurait fait quelque chose.
											Nous sommes stupides,
mais peut-être pas autant.
											Ou peut-être que si.
									","
											CA: No se podría argumentar que,
											si lo que hacemos hoy
											hubiera producido 10 º C de diferencia
en un periodo que pudiéramos ver,
											la humanidad ya se habría puesto las pilas
para hacer algo al respecto.
											Somos estúpidos, pero quizá no tanto.
											O tal vez sí.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I wouldn't bet on it.
									","
											NB : Je ne parierais pas.
									","
											NB: No apostaría por ello.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											You could imagine other features.
											So, right now, it's a little bit difficult
to switch to renewables and stuff, right,
											but it can be done.
											But it might just have been,
with slightly different physics,
											it could have been much more expensive
to do these things.
									","
											Vous pourriez imaginer d'autres options.
											Actuellement, c'est un peu difficile
de passer aux énergies renouvelables,
											mais c'est possible.
											Cela aurait pu être que,
avec une physique légèrement différente,
											il aurait été bien plus cher
de faire ces choses-là.
									","
											Puedes imaginarte otras características.
											En estos momentos es algo difícil cambiar
a las renovables y todo eso, de acuerdo,
											pero se puede hacer.
											Pero podría haber sucedido que,
con una física algo diferente,
											hubiera sido mucho más difícil
hacer estas cosas.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: And what's your view, Nick?
											Do you think, putting
these possibilities together,
											that this earth, humanity that we are,
											we count as a vulnerable world?
											That there is a death ball in our future?
									","
											CA : Quelle est votre opinion ?
											Pensez-vous, en réunissant
ces possibilités,
											cette Terre, l'humanité que nous sommes,
											nous sommes un monde vulnérable ?
											Qu'il y a une boule de la mort
dans notre futur ?
									","
											CA: ¿Qué opinas, Nick, crees que,
teniendo en cuenta todo esto,
											este planeta, nosotros como humanidad,
conformamos un mundo vulnerable,
											que hay una bola de muerte
en nuestro futuro?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: It's hard to say.
											I mean, I think there might
well be various black balls in the urn,
											that's what it looks like.
											There might also be some golden balls
											that would help us
protect against black balls.
											And I don't know which order
they will come out.
									","
											NB : C'est difficile à dire.
											Je pense qu'il pourrait y avoir
diverses boules noires dans l'urne,
											c'est ce qu'il semble.
											Il pourrait aussi y avoir
des boules dorées
											qui nous aideraient
à nous protéger des boules noires.
											Je ne sais pas dans quel ordre
elles vont sortir.
									","
											NB: Es difícil de decir,
											creo que también hay algunas bolas doradas
en la urna, o eso parece.
											Puede que haya algunas bolas doradas
											que nos puedan proteger
de las bolas negras.
											Y no sé en qué orden van a salir.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: I mean, one possible
philosophical critique of this idea
											is that it implies a view
that the future is essentially settled.
											That there either
is that ball there or it's not.
											And in a way,
											that's not a view of the future
that I want to believe.
											I want to believe
that the future is undetermined,
											that our decisions today will determine
											what kind of balls
we pull out of that urn.
									","
											CA : Une critique philosophique
possible de cette idée,
											c'est que cela implique une vision
de l'avenir qui est globalement établie.
											Soit cette boule est là,
soit elle ne l'est pas.
											D'une certaine façon,
ce n'est pas une vision de l'avenir
											en laquelle je veux croire.
											Je veux croire que l'avenir
est indéterminé,
											que nos décisions
aujourd'hui détermineront
											quel genre de boules
nous sortons de l'urne.
									","
											CA: Una posible crítica
filosófica a esta idea
											es que implica la visión de que el futuro
está básicamente determinado.
											De que o bien está esa bola ahí o no está.
											Y de alguna manera, no es una visión
del futuro en la que quiero creer.
											Quiero creer que el futuro
es indeterminado,
											que nuestras decisiones de hoy
determinarán qué tipos de bolas sacamos.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I mean, if we just keep inventing,
											like, eventually we will
pull out all the balls.
											I mean, I think there's a kind
of weak form of technological determinism
											that is quite plausible,
											like, you're unlikely
to encounter a society
											that uses flint axes and jet planes.
											But you can almost think
of a technology as a set of affordances.
											So technology is the thing
that enables us to do various things
											and achieve various effects in the world.
											How we'd then use that,
of course depends on human choice.
											But if we think about these
three types of vulnerability,
											they make quite weak assumptions
about how we would choose to use them.
											So a Type-1 vulnerability, again,
this massive, destructive power,
											it's a fairly weak assumption
											to think that in a population
of millions of people
											there would be some that would choose
to use it destructively.
									","
											NB : Si nous continuons à inventer,
											nous finirons par sortir
toutes les boules.
											Je pense qu'il y a une forme
de déterminisme technologique
											qui est assez plausible.
											Vous avez peu de probabilités
de rencontrer une société
											qui utilise des haches en silex
et des avions à réaction.
											Vous pouvez presque voir la technologie
comme un ensemble de potentialités.
											La technologie est la chose
nous permettant de faire diverses choses
											et d'obtenir divers effets sur le monde.
											Comment nous allons l'utiliser
dépend des choix humains.
											Mais si vous pensez
à ces trois types de vulnérabilités,
											elles ne supposent pas tant
de comment nous choisirions de l'utiliser.
											Une vulnérabilité de type 1,
cet énorme pouvoir destructeur,
											ce n'est pas une hypothèse forte
											de penser que dans une population
de millions de gens
											il y en aurait qui choisiraient
de l'utiliser pour détruire.
									","
											NB: Si seguimos inventando,
											al final acabaremos
sacando todas las bolas.
											Creo que hay una forma débil
de determinismo tecnológico
											que es bastante plausible,
											como que es bastante improbable
que encontremos una sociedad
											que use hachas de pedernal
y aviones tipo jet.
											Pero casi puedes pensar en una tecnología
como un conjunto de posibilidades.
											La tecnología nos permite
hacer varias cosas
											y conseguir diversos efectos en el mundo.
											Cómo la usemos depende
de nuestra elección como humanos.
											Pero si pensamos 
en estos tres tipos de vulnerabilidades,
											hacen unas asunciones bastante débiles
sobre cómo podríamos elegir utilizarla.
											En el Tipo 1 de vulnerabilidad,
con su poder destructor masivo,
											es una asunción bastante débil suponer
											que en una población
de millones de personas
											haya alguno que elija
usarlo de manera destructiva.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: For me, the most single
disturbing argument
											is that we actually might have
some kind of view into the urn
											that makes it actually
very likely that we're doomed.
											Namely, if you believe
in accelerating power,
											that technology inherently accelerates,
											that we build the tools
that make us more powerful,
											then at some point you get to a stage
											where a single individual
can take us all down,
											and then it looks like we're screwed.
											Isn't that argument quite alarming?
									","
											CA : Pour moi, l'argument
qui me perturbe le plus,
											c'est que nous pourrions
avoir un aperçu de l'urne
											qui rendrait très probable
que nous soyons condamnés.
											Si vous croyez au pouvoir accélérateur,
											qu'intrinsèquement,
la technologie accélère,
											que nous créons les outils
nous rendant plus puissants,
											vous finissez par en arriver à un point
											où chaque individu peut tous nous éliminer
											et il semble alors que nous soyons foutus.
											Cet argument n'est-il pas alarmant ?
									","
											CA: Para mí, el argumento más inquietante
											es que puede que tengamos
cierta visión de la urna,
											que hace pensar que muy posiblemente
estemos condenados.
											Por ejemplo, si crees
en el poder de la aceleración,
											esa tecnología inherentemente se acelera,
											y si creamos la herramientas
que nos den más poder
											llegaremos a un punto
											en el que un solo individuo
pueda acabar con todo,
											y eso nos lleva a pensar
que estamos apañados.
											¿No es un argumento bastante alarmante?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Ah, yeah.
									","
											NB : Oui.
									","
											NB: Ah, sí...
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											I think —
											Yeah, we get more and more power,
											and [it's] easier and easier 
to use those powers,
											but we can also invent technologies
that kind of help us control
											how people use those powers.
									","
											Je pense —
											Nous avons de plus en plus de pouvoir
											et il est de plus en plus simple
d'utiliser ce pouvoir,
											mais nous pouvons aussi
inventer des technologies
											qui nous aident à contrôler l'utilisation
que les gens font de ce pouvoir.
									","
											Creo que...
											Sí, tenemos más y más poder,
											y es cada vez más sencillo utilizarlo,
											pero también podemos inventar tecnologías
											que nos ayuden a controlar 
cómo las personas utilizan ese poder.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So let's talk about that,
let's talk about the response.
											Suppose that thinking
about all the possibilities
											that are out there now —
											it's not just synbio,
it's things like cyberwarfare,
											artificial intelligence, etc., etc. —
											that there might be
serious doom in our future.
											What are the possible responses?
											And you've talked about
four possible responses as well.
									","
											CA : Parlons de cela,
parlons de la réponse.
											Supposons que réfléchir
à toutes les possibilités
											qu'il y a actuellement —
											ce n'est pas que la biologie synthétique,
ce sont les cyberguerres,
											l'intelligence artificielle
et ainsi de suite —
											qu'il y a une condamnation
sérieuse dans notre avenir.
											Quelles sont les réponses possibles ?
											Vous avez parlé
de quatre réponses possibles.
									","
											CA: Hablemos de eso, de la respuesta.
											Imagina todas las posibilidades
que tenemos ahora,
											no es solo la biología sintética,
											sino cosas como la ciberguerra,
la inteligencia artificial, etc.,
											que indican que nuestro futuro
puede estar seriamente condenado.
											¿Cuáles son las posibles respuestas?
											Has hablado de cuatro posibles respuestas.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Restricting technological development
doesn't seem promising,
											if we are talking about a general halt
to technological progress.
											I think neither feasible,
											nor would it be desirable
even if we could do it.
											I think there might be very limited areas
											where maybe you would want
slower technological progress.
											You don't, I think, want
faster progress in bioweapons,
											or in, say, isotope separation,
											that would make it easier to create nukes.
									","
											NB : Restreindre
le développement technologique
											ne semble pas prometteur
											si nous parlons
d'une interruption générale
											des progrès technologiques.
											Ce n'est ni faisable ni désirable,
											même si nous le pouvions.
											Je pense qu'il y a
des domaines très limités
											où vous pourriez vouloir
ralentir les progrès technologiques.
											Vous ne voulez pas de progrès
plus rapides pour les armes biologiques
											ou la séparation d'isotope,
											ce qui faciliterait la création
de bombes nucléaires.
									","
											NB: Restringir el desarrollo tecnológico
no parece prometedor,
											si hablamos de un parón general
del progreso tecnológico,
											no creo que sea ni factible
ni deseable, aunque lo pudiéramos hacer.
											Creo que hay áreas muy limitadas
											en las que tal vez desees
un progreso tecnológico más lento.
											No creo que quieras un progreso
mayor en bioarmamento,
											o en separación isotópica,
											que facilitaría la creación
de armas nucleares.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: I mean, I used to be
fully on board with that.
											But I would like to actually
push back on that for a minute.
											Just because, first of all,
											if you look at the history
of the last couple of decades,
											you know, it's always been
push forward at full speed,
											it's OK, that's our only choice.
											But if you look at globalization
and the rapid acceleration of that,
											if you look at the strategy of
""move fast and break things""
											and what happened with that,
											and then you look at the potential
for synthetic biology,
											I don't know that we should
move forward rapidly
											or without any kind of restriction
											to a world where you could have
a DNA printer in every home
											and high school lab.
											There are some restrictions, right?
									","
											CA : Avant, j'étais
complètement d'accord avec cela.
											Mais j'aimerais prendre un instant
pour contre-argumenter.
											Tout d'abord,
											si vous considérez l'histoire
des dernières décennies,
											il y a toujours eu une évolution
vers l'avant à pleine vitesse,
											ça va, c'est notre seul choix.
											Mais si vous considérez la mondialisation
et son accélération rapide,
											si vous considérez la stratégie
d' « avancer vite et casser des trucs »
											et ce qui est arrivé,
											puis que vous considérez le potentiel
de la biologie synthétique,
											je ne sais pas si nous devrions
avancer rapidement
											ou sans aucune restriction
											vers un monde où vous pourriez avoir
une imprimante d'ADN à la maison
											et dans les lycées.
											Il y a des restrictions, non ?
									","
											CA: Antes solía estar
totalmente de acuerdo en eso.
											Pero me gustaría retroceder
un paso un momento.
											Antes de nada, si ves la historia 
de hace un par de décadas,
											ha sido una continua
aceleración a toda velocidad,
											está bien, pero es nuestra elección,
											pero si te fijas en la globalización
y su rápida aceleración,
											si te fijas en la estrategia
de ""muévete rápido y rompe cosas""
											y lo que ha producido,
											y luego te fijas en el potencia
de la biología sintética,
											no creo que debamos avanzar rápidamente
sin ningún tipo de restricción
											hacia un mundo en el que pueda haber
una impresora de ADN en cada casa
											en cada laboratorio de secundaria.
											Hay algunas restricciones, ¿verdad?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Possibly, there is
the first part, the not feasible.
											If you think it would be
desirable to stop it,
											there's the problem of feasibility.
											So it doesn't really help
if one nation kind of —
									","
											NB : Il y a la première partie :
l'infaisabilité.
											Si vous pensez qu'il serait
désirable d'arrêter,
											il y a le problème de faisabilité.
											Cela n'aide pas vraiment si une nation —
									","
											NB: Posiblemente en la primera parte,
en la no factibilidad,
											si crees que sería deseable pararla,
aparece el problema de la factibilidad.
											No ayudaría que un país...
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: No, it doesn't help
if one nation does,
											but we've had treaties before.
											That's really how we survived
the nuclear threat,
											was by going out there
											and going through
the painful process of negotiating.
											I just wonder whether the logic isn't
that we, as a matter of global priority,
											we shouldn't go out there and try,
											like, now start negotiating
really strict rules
											on where synthetic bioresearch is done,
											that it's not something
that you want to democratize, no?
									","
											CA : Cela n'aide pas si une nation arrête,
											mais nous avons eu des traités auparavant.
											C'est ainsi que nous avons survécu
à la menace nucléaire,
											c'est en en passant par le processus
douloureux des négociations.
											Je me demande si la logique n'est pas
que nous, en termes de priorité mondiale,
											nous ne devrions pas essayer,
											commencer à négocier maintenant
des règles très strictes
											sur les lieux de conduite
de recherches biologiques.
											Ce n'est pas une chose
que vous voulez démocratiser, si ?
									","
											CA: No, no ayudaría
que un país lo hiciera,
											pero hemos tenido
tratados con anterioridad.
											Así es como hemos sobrevivido
a la amenaza nuclear,
											gracias a que hemos salido y hemos pasado
por el penoso proceso de negociación.
											Me pregunto si la lógica no es acaso
que nosotros, como prioridad global,
											no deberíamos salir e intentar
negociar normas muy duras
											sobre dónde se hace la investigación
en biología sintética,
											porque no es algo
que quieras democratizar, ¿no?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I totally agree with that —
											that it would be desirable, for example,
											maybe to have DNA synthesis machines,
											not as a product where each lab
has their own device,
											but maybe as a service.
											Maybe there could be
four or five places in the world
											where you send in your digital blueprint
and the DNA comes back, right?
											And then, you would have the ability,
											if one day it really looked
like it was necessary,
											we would have like,
a finite set of choke points.
											So I think you want to look
for kind of special opportunities,
											where you could have tighter control.
									","
											NB : Je suis complètement d'accord —
											il serait désirable, par exemple,
											d'avoir des machines de synthèse d'ADN,
											pas en tant que produit pour lequel
chaque labo a son appareil,
											mais en tant que service.
											Il pourrait y avoir
quatre ou cinq endroits au monde
											où vous pourriez envoyer
un modèle numérique
											et l'ADN arriverait à vous.
											Vous auriez la capacité,
											si un jour cela semblait
vraiment nécessaire,
											il y aurait un ensemble fini
de goulots d'étranglement.
											Je pense que vous voulez considérer
des opportunités particulières
											où vous pourriez avoir
un contrôle plus strict.
									","
											NB: Estoy totalmente de acuerdo con eso,
											sería deseable, por ejemplo,
											tener máquinas para sintetizar ADN,
											no de manera en la que cada laboratorio
sea el dueño de su propio dispositivo,
											sino como un servicio.
											Podría haber 
cuatro o cinco lugares en el mundo
											a los que enviar el modelo digital
y recibir el ADN de vuelta, ¿verdad?
											Entonces, tendrías la habilidad,
si un día fuera necesario,
											tendríamos un número finito
de cuellos de botella.
											Creo que tienes que fijarte
en ciertos tipos de oportunidades
											sobre las que quieres tener control.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Your belief is, fundamentally,
											we are not going to be successful
in just holding back.
											Someone, somewhere —
North Korea, you know —
											someone is going to go there
and discover this knowledge,
											if it's there to be found.
									","
											CA : Vous croyez, au fond,
											que nous ne réussirons pas
à contenir les choses.
											Quelqu'un, quelque part —
en Corée du Nord —
											quelqu'un va y arriver
et découvrir ce savoir,
											s'il est à découvrir.
									","
											CA: Crees, fundamentalmente,
											que no vamos a conseguir retenerlo.
											Que alguien, en algún lugar
como Corea del Norte,
											va a descubrir este conocimiento,
si es que es posible descubrirlo.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: That looks plausible
under current conditions.
											It's not just synthetic biology, either.
											I mean, any kind of profound,
new change in the world
											could turn out to be a black ball.
									","
											NB : C'est plausible
dans les conditions actuelles.
											Ce n'est pas que la biologie synthétique.
											N'importe quel genre
de changement profond et nouveau
											pourrait être une boule noire.
									","
											NB: Parece posible
en las condiciones actuales.
											No es solo la biología sintética,
											sino que cualquier cambio
nuevo y profundo en el mundo
											puede convertirse en una bola negra.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Let's look at 
another possible response.
									","
											CA : Considérons
une autre réponse possible.
									","
											CA: Vamos a ver otra posible respuesta.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: This also, I think,
has only limited potential.
											So, with the Type-1 vulnerability again,
											I mean, if you could reduce the number
of people who are incentivized
											to destroy the world,
											if only they could get
access and the means,
											that would be good.
									","
											NB : Ceci, à mon avis,
a un potentiel limité.
											Avec la vulnérabilité de type 1,
											si vous pouvez réduire le nombre
de personnes qui sont motivées
											à détruire le monde,
											si elles seules avaient
l'accès et les moyens,
											ce serait bien.
									","
											NB: Creo que esto también
tiene un potencial limitado.
											Con el Tipo 1 de vulnerabilidad,
											si puedes reducir el número
de personas motivadas a destruir el mundo,
											si solo ellos tuvieran
acceso a los medios,
											eso estaría bien.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: In this image that you asked us to do
											you're imagining these drones
flying around the world
											with facial recognition.
											When they spot someone
showing signs of sociopathic behavior,
											they shower them with love, they fix them.
									","
											CA : Dans cette image,
											vous imaginez ces drones
volant à travers le monde
											avec une reconnaissance faciale.
											Quand ils repèrent quelqu'un
présentant un comportement sociopathe,
											ils le couvrent d'amour, le rétablissent.
									","
											CA: En esta imagen que nos muestras,
											imaginas esos drones de reconocimiento
facial volando alrededor del mundo.
											Cuando detectan a alguien
con signos de comportamiento sociopático,
											lo cubren de amor y lo curan.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I think it's like a hybrid picture.
											Eliminate can either mean,
like, incarcerate or kill,
											or it can mean persuade them
to a better view of the world.
											But the point is that,
											suppose you were
extremely successful in this,
											and you reduced the number
of such individuals by half.
											And if you want to do it by persuasion,
											you are competing against
all other powerful forces
											that are trying to persuade people,
											parties, religion, education system.
											But suppose you could reduce it by half,
											I don't think the risk
would be reduced by half.
											Maybe by five or 10 percent.
									","
											NB : C'est une image hybride.
											Éliminer peut signifier
soit incarcérer ou tuer,
											soit les persuader d'avoir
une meilleure vision du monde.
											Le principe, c'est :
											imaginez être extrêmement performant
											et réduire le nombre
de tels individus de moitié.
											Si vous voulez le faire via la persuasion,
											vous rivalisez contre
toutes les grandes forces
											qui essayent de persuader les gens,
											les partis, la religion,
le système éducatif.
											Supposez pouvoir le réduire de moitié,
											le risque ne serait pas réduit de moitié.
											Peut-être de 5% ou 10%.
									","
											NB: Creo que es una imagen híbrida.
											Eliminar puede significar
encarcelar o matar,
											o persuadir de una visión
más optimista del mundo.
											Pero el asunto es,
											imagina que tuvieras éxito
											y redujeras el número
de esos individuos a la mitad.
											Si lo quieres hacer
mediante la persuasión,
											estás compitiendo
contra otras fuerzas poderosas
											que tratan de persuadir a la gente:
											partidos políticos, religión
o el sistema educativo.
											Pero imagina que lo reduces a la mitad;
											no creo que el riesgo
se redujera a la mitad,
											sino a un 5 o 10 %.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: You're not recommending that we gamble
humanity's future on response two.
									","
											CA : Vous ne recommandez pas
que l'on parie l'avenir de l'humanité
											sur la seconde réponse.
									","
											CA: No recomiendas que nos juguemos
el futuro de la humanidad
											a la respuesta 2.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I think it's all good
to try to deter and persuade people,
											but we shouldn't rely on that
as our only safeguard.
									","
											NB : C'est bien d'essayer
de dissuader et persuader les gens,
											mais pas miser dessus
comme seul dispositif de sécurité.
									","
											NB: Creo que está muy bien
tratar de disuadir a las personas,
											pero no deberíamos considerarlo
nuestro único salvavidas.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: How about three?
									","
											CA : Et la troisième réponse ?
									","
											CA: ¿Y la tercera?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I think there are two general methods
											that we could use to achieve
the ability to stabilize the world
											against the whole spectrum
of possible vulnerabilities.
											And we probably would need both.
											So, one is an extremely effective ability
											to do preventive policing.
											Such that you could intercept.
											If anybody started to do
this dangerous thing,
											you could intercept them
in real time, and stop them.
											So this would require
ubiquitous surveillance,
											everybody would be monitored all the time.
									","
											NB : Il y a deux méthodes générales
											que nous pourrions utiliser pour parvenir
à la capacité de stabiliser le monde
											face à l'éventail complet
des vulnérabilités possibles.
											Nous aurions besoin des deux.
											Une est une capacité optimale
											de prévention policière.
											Afin de pouvoir intercepter.
											Si quiconque entamait
cette chose dangereuse,
											vous pourriez l'intercepter
en temps réel et l'arrêter.
											Cela nécessiterait
une surveillance omniprésente,
											tout le monde serait
constamment surveillé.
									","
											NB: Creo que hay tres métodos generales
											con los que que podemos tener
la habilidad de estabilizar el mundo
											contra todo el espectro
de posibles vulnerabilidades.
											Y probablemente necesitamos las dos.
											Una es una habilidad extremadamente
efectiva de crear políticas preventivas,
											pensadas para interceptar.
											Si todos empezaran
a hacer esto tan peligroso,
											podrías interceptarlos
en tiempo real y detenerlos.
											Esto requeriría vigilancia ubicua,
											todo el mundo estaría
monitorizado todo el tiempo.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: This is ""Minority Report,""
essentially, a form of.
									","
											CA : C'est en gros
une forme de « Minority Report ».
									","
											CA: Esto es ""Minority Report"", en esencia.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: You would have maybe AI algorithms,
											big freedom centers
that were reviewing this, etc., etc.
									","
											NB : Vous auriez des algorithmes d'IA,
											de gros centres de liberté
qui examineraient cela, et ainsi de suite.
									","
											NB: Tal vez tendrías algoritmos de IA,
											grandes centros de libertad
que estarían revisándolos, etc.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: You know that mass surveillance
is not a very popular term right now?
									","
											CA : La surveillance de masse
n'est pas très populaire en ce moment.
									","
											CA: ¿Sabes que la vigilancia masiva
											no es un término
muy popular en este momento?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Yeah, so this little device there,
											imagine that kind of necklace
that you would have to wear at all times
											with multidirectional cameras.
											But, to make it go down better,
											just call it the ""freedom tag""
or something like that.
									","
											NB : Ce petit dispositif,
											imaginez ce genre de collier
que vous devriez porter tout le temps,
											avec des caméras multidirectionnelles.
											Mais pour que cela passe mieux,
											appelez-le la « balise
de la liberté » ou autre.
									","
											NB: Sí, ese pequeño dispositivo,
											imagina un tipo de collar
que tuvieras que llevar todo el tiempo
											con cámaras multidireccionales.
											Pero para hacerlo más llevadero,
											imagínate llamarlo
""etiqueta de libertad"" o algo así.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: OK.
											I mean, this is the conversation, friends,
											this is why this is
such a mind-blowing conversation.
									","
											CA : D'accord.
											C'est la conversation —
											c'est pour cela que c'est
une conversation si époustouflante.
									","
											CA: De acuerdo.
											Esta es la conversación, amigos,
											esta es la razón por la que es
una conversación alucinante.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Actually, there's
a whole big conversation on this
											on its own, obviously.
											There are huge problems and risks
with that, right?
											We may come back to that.
											So the other, the final,
											the other general stabilization capability
											is kind of plugging
another governance gap.
											So the surveillance would be kind of
governance gap at the microlevel,
											like, preventing anybody
from ever doing something highly illegal.
											Then, there's a corresponding
governance gap
											at the macro level, at the global level.
											You would need the ability, reliably,
											to prevent the worst kinds
of global coordination failures,
											to avoid wars between great powers,
											arms races,
											cataclysmic commons problems,
											in order to deal with
the Type-2a vulnerabilities.
									","
											NB : Il y a toute une conversation
à ce sujet-là en particulier.
											Il y a de gros problèmes
et risques avec cela.
											Nous y reviendrons peut-être.
											L'autre, la dernière,
											l'autre capacité de stabilisation générale
comble un autre vide dans la gouvernance.
											La surveillance comblerait pour un vide
dans la gouvernance à un niveau micro :
											empêcher quiconque de faire
quoi que ce soit de hautement illégal.
											Il y un vide équivalent
dans la gouvernance
											au niveau macro, au niveau mondial.
											Il vous faudrait la capacité fiable
											d'empêcher les pires types
d'échecs de coordination mondiale,
											d'éviter les guerres
entre les grandes puissances,
											les courses à l'armement,
											les problèmes communs cataclysmiques,
											afin de faire face
aux vulnérabilités de type 2a.
									","
											NB: De hecho, ya hay una gran conversación
sobre de esto, obviamente.
											Entraña grandes problemas
y riesgos, ¿verdad?
											Tal vez volvamos a eso.
											Así que última, la otra capacidad
de estabilización general
											es llenar otro vacío de gobernanza.
											De modo que la vigilancia sería
un vacío de gobernanza a nivel micro,
											como impedir que cualquiera
haga algo altamente ilegal.
											Pero hay un vacío de gobernanza
a nivel macro, global.
											Necesitarías la habilidad,
de manera fiable,
											de prevenir los mayores fallos
de coordinación global,
											para evitar guerras entre grandes poderes,
											guerras armamentísticas,
											problemas cataclísmicos
de los bienes comunes,
											para poder lidiar
con las vulnerabilidades del Tipo 2a.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: Global governance is a term
											that's definitely way out
of fashion right now,
											but could you make the case
that throughout history,
											the history of humanity
											is that at every stage
of technological power increase,
											people have reorganized
and sort of centralized the power.
											So, for example,
when a roving band of criminals
											could take over a society,
											the response was,
well, you have a nation-state
											and you centralize force,
a police force or an army,
											so, ""No, you can't do that.""
											The logic, perhaps, of having
a single person or a single group
											able to take out humanity
											means at some point
we're going to have to go this route,
											at least in some form, no?
									","
											CA : La gouvernance mondiale est un terme
											qui n'est plus du tout
à la mode actuellement,
											mais vous pourriez avancer
qu'au cours de l'histoire,
											de l'histoire de l'humanité,
											à chaque moment de montée
du pouvoir technologique,
											les gens se sont réorganisés
et ont centralisé le pouvoir.
											Par exemple, quand un groupe
de criminels itinérants
											pouvait prendre le contrôle d'une société,
											la réponse a été d'avoir un État-nation
											et une force centralisée,
policière ou militaire,
											pour que ce ne soit pas possible.
											La logique d'avoir une seule personne
ou un seul groupe
											capable d'éradiquer l'humanité
											signifie qu'à un moment,
nous devrons emprunter ce chemin,
											sous une certaine forme, non ?
									","
											CA: La gobernanza global es un término
totalmente en desuso ahora,
											pero se podría argumentar
											que a lo largo de la historia
de la humanidad,
											en cada fase del aumento
del poder tecnológico,
											la gente se ha reorganizado
para centralizar el poder, por así decir,
											Por ejemplo, cuando una banda
errante de criminales
											podía conquistar una sociedad,
											se respondía con el Estado nación,
											y se centralizaba la fuerza,
policial o militar,
											así que ""no, no puedes hacer eso"".
											La lógica de que una sola persona o grupo
pueda conquistar la humanidad
											tal vez signifique que en algún momento
tendremos que tomar este camino,
											al menos en cierta manera, ¿no?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: It's certainly true that the scale
of political organization has increased
											over the course of human history.
											It used to be hunter-gatherer band, right,
											and then chiefdom, city-states, nations,
											now there are international organizations
and so on and so forth.
											Again, I just want to make sure
											I get the chance to stress
											that obviously there are huge downsides
											and indeed, massive risks,
											both to mass surveillance
and to global governance.
											I'm just pointing out
that if we are lucky,
											the world could be such
that these would be the only ways
											you could survive a black ball.
									","
											NB : Il est vrai que l'échelle
de l'organisation politique a augmenté
											au cours de l'histoire humaine.
											C'était un groupe de chasseurs-cueilleurs,
											puis des chefferies,
des cité-États, des nations,
											maintenant il y a des organisations
internationales et ainsi de suite.
											Je veux juste m'assurer
											d'avoir l'opportunité d'insister
											qu'il y a évidemment
d'énormes inconvénients
											et d'énormes risques
											dans la surveillance de masse
et la gouvernance mondiale.
											Je vais faire remarquer
que si nous sommes chanceux,
											le monde pourrait être tel
que ce seraient les seules façons
											de survivre à une boule noire.
									","
											NB: Es cierto que la escala
de la organización política ha aumentado
											a lo largo de la historia de la humanidad.
											Solíamos ser grupos 
de cazadores-recolectores,
											luego liderados por jefes,
Estados naciones, naciones,
											y ahora hay organizaciones
internacionales y todo eso.
											De nuevo, solo quiero asegurarme
											de que remarco que obviamente
hay enormes desventajas,
											y desde luego, riesgos descomunales,
											tanto en la vigilancia masiva
como en la gobernanza global.
											Solo estoy destacando que,
si tenemos suerte,
											el mundo puede llegar a un punto
											en el que solo así podamos
sobrevivir a la bola negra.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: The logic of this theory,
											it seems to me,
											is that we've got to recognize
we can't have it all.
											That the sort of,
											I would say, naive dream
that many of us had
											that technology is always
going to be a force for good,
											keep going, don't stop,
go as fast as you can
											and not pay attention
to some of the consequences,
											that's actually just not an option.
											We can have that.
											If we have that,
											we're going to have to accept
											some of these other
very uncomfortable things with it,
											and kind of be in this
arms race with ourselves
											of, you want the power,
you better limit it,
											you better figure out how to limit it.
									","
											CA : La logique de cette théorie,
											me semble-t-il,
											c'est que nous devons reconnaître
que nous ne pouvons pas tout avoir.
											C'est le genre
											de rêve naïf, si je puis dire,
que nombre d'entre nous avaient
											que la technologie sera toujours
une force du bien,
											qu'il ne faut ne pas s'arrêter,
aller aussi vite que possible
											et ne pas prêter attention
à certaines conséquences,
											ce n'est même pas une option.
											Nous pouvons le faire.
											Si nous le faisons,
											il faudra accepter
											certaines choses très désagréables
											et faire la course à l'armement
avec nous-mêmes :
											si vous voulez le pouvoir,
vous devriez le confiner,
											vous devriez trouver comment le confiner.
									","
											CA: La lógica de esta teoría,
											me parece,
											es que tenemos que asumir
que no podemos tenerlo todo.
											Que el sueño inocente, por así decir,
que muchos de nosotros teníamos
											de que esa tecnología siempre
va a ser una fuerza positiva,
											sigue adelante, no pares,
ve tan rápido como puedas
											y no te fijes en las consecuencias,
											ya no es una opción.
											Podemos tenerlo.
											Si podemos tenerlo,
											tendremos que aceptar algunos aspectos
muy incómodos que conlleva,
											y entrar esta carrera armamentística
con nosotros mismos,
											de modo que si quieres poder
tienes que limitarlo,
											y más te vale averiguar cómo.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: I think it is an option,
											a very tempting option,
it's in a sense the easiest option
											and it might work,
											but it means we are fundamentally
vulnerable to extracting a black ball.
											Now, I think with a bit of coordination,
											like, if you did solve this
macrogovernance problem,
											and the microgovernance problem,
											then we could extract
all the balls from the urn
											and we'd benefit greatly.
									","
											NB : Je pense que c'est une option,
											une option très tentante,
dans un sens la plus simple
											et cela pourrait fonctionner,
											mais nous sommes alors vulnérables
à l'extraction d'une boule noire.
											Avec un peu de coordination —
											en résolvant ce problème
de macro gouvernance
											et celui de micro gouvernance —
											nous pourrions extraire
toutes les boules de l'urne
											et nous en bénéficierons grandement.
									","
											NB: Creo que es una opción,
											una opción muy tentadora,
diría que la más sencilla,
											y podría funcionar,
											pero significa que somos fundamentalmente
vulnerables a extraer la bola negra.
											Pero pienso que con un poco
de coordinación,
											pongamos que has resuelto
el problema de la macrogobernanza
											y el de la microgobernanza,
											podríamos extraer
todas las bolas de la urna
											y beneficiarnos enormemente.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: I mean, if we're living
in a simulation, does it matter?
											We just reboot.
									","
											CA : Si nous vivons dans une simulation,
cela importe-t-il ?
											Nous recommençons.
									","
											CA: Y si estamos viviendo
en una simulación, ¿qué más da?
											Simplemente nos reiniciamos.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: Then ... I ...
									","
											NB : Alors... je...
									","
											NB: Entonces...
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
											I didn't see that one coming.
									","
											(Rires)
											Je ne l'ai pas vu arriver.
									","
											(Risas)
											Eso no me lo esperaba.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: So what's your view?
											Putting all the pieces together,
how likely is it that we're doomed?
									","
											CA : Quelle est votre opinion ?
											En rassemblant toutes les pièces,
											quelle est notre probabilité
d'être condamnés ?
									","
											CA: ¿Cuál es tu opinión?
											Si juntamos todas las piezas,
											qué posibilidades tenemos
de estar condenados?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											I love how people laugh
when you ask that question.
									","
											J'adore comment les gens rient
à cette question.
									","
											Me encanta que la gente se ría
cuando hago esa pregunta.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: On an individual level,
											we seem to kind of be doomed anyway,
just with the time line,
											we're rotting and aging
and all kinds of things, right?
									","
											NB : A un niveau individuel,
											nous semblons condamnés,
juste d'un point de vue de l'échéance,
											nous nous décomposons et vieillissons.
									","
											NB: A nivel individual,
											parece que ya estamos condenados,
solo con el cronograma,
											nos pudrimos y envejecemos
y todo eso, ¿verdad?
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											It's actually a little bit tricky.
											If you want to set up
so that you can attach a probability,
											first, who are we?
											If you're very old,
probably you'll die of natural causes,
											if you're very young,
you might have a 100-year —
											the probability might depend
on who you ask.
											Then the threshold, like, what counts
as civilizational devastation?
											In the paper I don't require
an existential catastrophe
											in order for it to count.
											This is just a definitional matter,
											I say a billion dead,
											or a reduction of world GDP by 50 percent,
											but depending on what
you say the threshold is,
											you get a different probability estimate.
											But I guess you could
put me down as a frightened optimist.
									","
											C'est délicat.
											Si vous voulez y attacher une probabilité,
											tout d'abord, qui sommes-nous ?
											Si vous êtes vieux,
vous mourrez de causes naturelles.
											Si vous êtes jeune,
vous avez 100 ans peut-être,
											selon à qui vous demandez.
											Le seuil : qu'est-ce qui compte
comme un dévaste civilisationnel ?
											Dans l'article, je n'ai pas besoin
d'une catastrophe civilisationnelle
											pour que cela compte.
											C'est une question de définition.
											Je dis un milliard de morts
											ou une diminution du PIB mondial de 50%,
											mais selon le seuil que vous donnez,
											vous aurez une probabilité différente.
											Vous pourriez dire que je suis
un optimiste effrayé.
									","
											La verdad es que es un poco complicado.
											Si quieres hacerte una idea
para obtener una probabilidad,
											antes de nada, ¿quiénes somos?
											Si eres muy mayor, seguramente
morirás de causas naturales,
											si eres muy joven, tal vez vivas 100 años;
											la probabilidad dependerá
de a quién preguntes.
											Luego está el umbral: ¿Qué se considera
como devastación de la humanidad?
											Sobre el papel, no necesito
una catástrofe existencial
											para que la considere como tal.
											Es cuestión de definición:
											si digo mil millones de muertos,
o la reducción del PIB al 50 %,
											así que dependiendo
de dónde pongas el umbral,
											obtienes una estimación
de probabilidad diferente.
											Supongo que me podrías catalogar
como un optimista asustado.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: You're a frightened optimist,
											and I think you've just created
a large number of other frightened ...
											people.
									","
											CA : Vous êtes un optimiste effrayé
											et vous venez de créer un grand nombre
d'autres personnes effrayées.
									","
											CA: Eres un optimista asustado,
											y creo que acabas de crear
una gran cantidad de personas asustadas...
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Laughter)
									","
											(Rires)
									","
											(Risas)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											NB: In the simulation.
									","
											NB : Dans la simulation.
									","
											NB: En la simulación.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											CA: In a simulation.
											Nick Bostrom, your mind amazes me,
											thank you so much for scaring
the living daylights out of us.
									","
											CA : Dans une simulation.
											Nick Bostrom, votre esprit m'épate.
											Merci beaucoup de nous avoir effrayés.
									","
											CA: En una simulación.
											Nick Bostrom, tu mente me fascina,
											muchas gracias por asustarnos
hasta la médula.
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
											(Applause)
									","
											(Applaudissements)
									","
											(Aplausos)
									",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
"
","
","
",How civilization could destroy itself — and 4 ways we could prevent it,Nick Bostrom,21:09,"future,technology,machine learning,AI,vulnerability,life,philosophy,humanity,society"
